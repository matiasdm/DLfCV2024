{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Deep Learning. \n",
        "\n",
        "We move from classic classifiers to deep learning based architectures. In this practical session we will cover:\n",
        "- How to implement a simple neural network using PyTorch.\n",
        "- How to train a neural network and check different optimization strategies.\n",
        "- Review the concept of overfitting and how to avoid it using regularization techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We loaded 50000 training, 10000 validation, and 10000 testing samples\n"
          ]
        }
      ],
      "source": [
        "# Let's start by importing torch (you might need to pip install if you haven't already)\n",
        "import torch \n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Get the MNIST dataset\n",
        "# Set up the data preprocessing and loading:\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])  # transform the data to torch tensor and normalize\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Now let's split the training set into a training and validation set\n",
        "generator = torch.Generator().manual_seed(42)  # just a random generator\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [50000, 10000], generator=generator)\n",
        "\n",
        "print('We loaded {} training, {} validation, and {} testing samples'.format(len(train_dataset), len(val_dataset), len(test_dataset)))\n",
        "\n",
        "# Set up the data loaders (they are iterable objects that return the data in batches)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize some of the data \n",
        "# --------------------------------------------------------------\n",
        "# Excercise 1: Visualize the first 6 images and their labels from the test set\n",
        "# --------------------------------------------------------------\n",
        "#TODO\n",
        "# ---------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Excersice 2: what is the size of the input images? If we want to use a fully connected layer, what is the size of the input of the fully connected layer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (logsoftmax): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Let's define the neural network architecture to learn how to classify the images\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model \n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return self.logsoftmax(x)\n",
        "\n",
        "print(Net())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Excersice 3: Draw a schematic representation of the model, how many trainable parameters does it have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's visualize the model graph with tensorboard\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter(\"torchlogs/\")\n",
        "model = Net()\n",
        "batch_idx, (images, labels) = next(examples)\n",
        "writer.add_graph(model, images)\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now let's train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.298582\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.077154\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.714908\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.281494\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.638875\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.770361\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.667440\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.535242\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.523662\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.447204\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.428013\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.396825\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.395202\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.404668\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.208781\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.309498\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.406914\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.319719\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.325976\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.402554\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.374491\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.262554\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.234488\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.352436\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.309471\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.298479\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.444830\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.340116\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.168590\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.378547\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.261132\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.242201\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.510193\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.290604\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.411796\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 0.211007\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.570498\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.385725\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.143316\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 0.451585\n"
          ]
        }
      ],
      "source": [
        "# Now let's train the model\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 5  # an epoch is a full pass through the dataset (each step is one batch, once we used all the batches we have completed an epoch)\n",
        "model = Net()  # init the netwokr\n",
        "criterion = nn.CrossEntropyLoss()  # define the loss function as the cross entropy loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # define the optimizer (in this case stochastic gradient descent)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # set the model to training mode\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):  # Iteare over the batches\n",
        "        optimizer.zero_grad()  # zero the gradients (otherwise they accumulate)\n",
        "        output = model(images)  # forward pass (compute the output)\n",
        "        loss = criterion(output, labels)  # compute the loss    \n",
        "        loss.backward()  # backward pass (compute the gradients and do backpropagation)\n",
        "        optimizer.step()  # update the weights (do one step of optimization)\n",
        "\n",
        "        if batch_idx % 100 == 0:  # print some information every 100 batches\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch+1, batch_idx * len(images), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's do the same but using tensorboard to visualize the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's repeat the training but using tensorboard to log the loss\n",
        "writer = SummaryWriter(\"torchlogs/experiment_ex3\")\n",
        "\n",
        "num_epochs = 5  # an epoch is a full pass through the dataset (each step is one batch, once we used all the batches we have completed an epoch)\n",
        "model = Net()  # init the netwokr\n",
        "criterion = nn.CrossEntropyLoss()  # define the loss function as the cross entropy loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # define the optimizer (in this case stochastic gradient descent)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # set the model to training mode\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()  # zero the gradients\n",
        "        output = model(images)  # forward pass\n",
        "        loss = criterion(output, labels)  # compute the loss\n",
        "        loss.backward()  # backward pass\n",
        "        optimizer.step()  # update the weights\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            writer.add_scalar('training loss', loss.item(), epoch*len(train_loader)+batch_idx)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Excersice 4: Repeat the training process, but now report the accuracy on the training and the validation set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's repeat the training but using tensorboard to log the loss\n",
        "writer = SummaryWriter(\"torchlogs/experiment_ex4\")\n",
        "\n",
        "num_epochs = 10  # an epoch is a full pass through the dataset (each step is one batch, once we used all the batches we have completed an epoch)\n",
        "model = Net()  # init the netwokr\n",
        "criterion = nn.CrossEntropyLoss()  # define the loss function as the cross entropy loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # define the optimizer (in this case stochastic gradient descent)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # set the model to training mode\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()  # zero the gradients\n",
        "        output = model(images)  # forward pass\n",
        "        loss = criterion(output, labels)  # compute the loss\n",
        "        loss.backward()  # backward pass\n",
        "        optimizer.step()  # update the weights\n",
        "\n",
        "        if batch_idx % 1000 == 0:\n",
        "            writer.add_scalar('training loss', loss.item(), epoch*len(train_loader)+batch_idx)\n",
        "            # ---------------------------------------------------------------\n",
        "            # Excercise 4: add two additional scalars to monitor the validation and training accuracy\n",
        "            #TODO\n",
        "            # ---------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Excersice 5: Compare the performance if you use only 100, 500, 1000, 10000, and 50000 samples for training. Do any of this models overfit?\n",
        "### Excersice 6: Add dropout layers and L2 regularization to avoid overfitting and repeat the previous experiments. Comment on the observed results. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
